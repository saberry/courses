---
title: "The General Linear Model"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
```

The general linear model is one of the foundations of statistical inference.

An important thing that you need to know is that it comprises three major techniques:

**Regression** tests the effects of variables on a dependent variable and will produce a predicted value for each observation.

**t-tests** are often used to test for differences between groups (or between a population and sample)

**Analysis of variance** can be used to test mean differences between more than 2 groups. 

## Regression

Linear regression is one of the most powerful statistical techniques. 

Let's look at the following plot:

```{r, echo = FALSE}
library(ggplot2)

x = rnorm(1000)

y = x + rnorm(1000)

xyDat = data.frame(x = x, y = y)

ggplot(xyDat, aes(x, y)) +
  geom_point(alpha = .75) +
  theme_minimal()
```

To perform our regression analysis, we need to make a line pass through the data to satisfy the following conditions:

1.  It has to pass through the mean of *x*.

2.  It has to pass through the mean of *y*.

3.  The slope of the line needs to minimize the distance between the line and observations.

Here is a plot with all of those points marked.

```{r, echo = FALSE}
library(dplyr)

datSummary = xyDat %>% 
  summarize_all(mean)

xyMod = lm(y ~ x, data = xyDat)

xyModDat = data.frame(resid = xyMod$fitted.values, 
                      fit = xyMod$model$x)

ggplot(xyDat, aes(x, y)) +
  geom_point(alpha = .75) +
  geom_point(data = datSummary, mapping = aes(x, y), color = "#ff5501", size = 5) +
  geom_hline(yintercept = datSummary$y, linetype = "dashed", color = "#ff5501") +
  geom_vline(xintercept = datSummary$x, linetype = "dashed", color = "#ff5501") +
  theme_minimal()
```


With those identified, we can fit a line through those points.

```{r, echo = FALSE}
ggplot(xyDat, aes(x, y)) +
  geom_point(alpha = .75) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = datSummary, mapping = aes(x, y), color = "#ff5501", size = 5) +
  geom_hline(yintercept = datSummary$y, linetype = "dashed", color = "#ff5501") +
  geom_vline(xintercept = datSummary$x, linetype = "dashed", color = "#ff5501") +
  theme_minimal()
```

The regression line gets adjusted so that it has the shortest squared distance between each and every point and the line. To be a little more specific, the distance between the line and the point gets squared and each of those squared values gets summed -- these are called the sum of squares.

You should definitely play with <a href="http://setosa.io/ev/ordinary-least-squares-regression/">this</a>!

## Points and Lines

Now that we have these points and lines, what can we make of them? The goal of our regression model is to account for the variation in the dependent variable with the predictor variable. To that end, we have three different types of variation in our model:

1.  Total variation in *Y*: $Y-\bar{Y}$

2.  Explained variation in *Y*: $\hat{Y} - \bar{Y}$

3.  Unexplained variation in *Y* $Y - \hat{Y}$


```{r, echo = FALSE}
ggplot(xyDat, aes(x, y)) +
  geom_point(alpha = .75) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = datSummary, mapping = aes(x, y), color = "#ff5501", size = 5) +
  geom_hline(yintercept = datSummary$y, linetype = "dashed", color = "#ff5501") +
  geom_vline(xintercept = datSummary$x, linetype = "dashed", color = "#ff5501") +
  geom_segment(data = xyModDat, mapping = aes(xend = fit, yend = resid), linetype = "dashed") +
  theme_minimal()
```


This is just a $\LaTeX$ emoji: $\hat{} - \bar{}$

## Important Terms

There are a few important terms to keep in mind:

-  Coefficients

-  Standard Errors

-  Residuals

## Goodness Of Fit

- $R^2$

-  *F*-tests

### $R^2$

The $R^2$ will give us the proportion of total explained variation.

Sums of squares play a very important roll in all of this.

$$\Sigma(y_i-\bar{y})^2 = \Sigma(\hat{y}-\bar{y})^2 + \Sigma(y - \hat{y})^2 $$

If we divide the explained sum of squares ($\Sigma(\hat{y}-\bar{y})^2$) by the total sum of squares part of the equation ($\Sigma(y_i-\bar{y})^2$), we will get the $r^2$:

$$r^2 = \frac {\Sigma(\hat{y}-\bar{y})^2}{\Sigma(y_i-\bar{y})^2}$$

It can alternatively be expressed as:

$$R^2 = 1 -\frac {\Sigma(y - \hat{y})^2} {\Sigma(y_i-\bar{y})^2}$$

If you have a bivariate model, it is literally $r^2$

```{r}
x = rnorm(1000)

y = x + rnorm(1000)

xyDat = data.frame(x = x, y = y)

xyMod = lm(y ~ x, data = xyDat)

summary(xyMod)

totalSS = sum((xyMod$model$y - mean(xyMod$model$y))^2)

explainedSS = sum((xyMod$fitted.values - mean(xyMod$model$y))^2)

residualSS = sum((xyMod$model$y - xyMod$fitted.values)^2)

r2 = explainedSS / totalSS

r2
```

### *F*-test

The *F*-test is used to determine whether the amount of variation explained is not due to chance. It is essentially testing whether our $R^2$ is "significant".

Our *F* is calculated as follows:

$$F = \frac {(\Sigma(y - \hat{y})^2)/v-1} {(\Sigma(\hat{y}-\bar{y})^2)/n-2}$$

We already know most of this, the top part has the residual sums of squares and the bottom part is the explained sums of squares. *v* is the number of variables and *n* is the sample size.

```{r}
fStat = (residualSS / 1) / (explainedSS / (nrow(xyDat) - 2))
```

### *t*-test

We use the *t*-test to determine if the slope of our line is different than zero. We are essentially testing each of our $\beta$ (coefficients) for significant slope.

## Regression Assumptions

As with nearly all statistical tests, the general linear model has some assumptions built into it for proper inference.

- Linear relationship between variables:

```{r, echo = FALSE}
plot(x, y)
```

- Multivariate normality within errors:

```{r, echo = FALSE}
testMod = lm(y ~ x)

hist(testMod$residuals)
```

- Little multicollinearity (relationships between predictor variables):

```{r, echo = FALSE}
plot(rnorm(100), rnorm(100))
```

- Homoscedasticity (errors should be randomly distributed):

```{r}
plot(testMod$fitted.values, testMod$residuals)
```

- No auto-correlations (time 1 does not influence time 2).

Assumptions are very important. They are not, however, statistical death sentences.

### Assumptions According To Gelman

Andrew Gelman has proposed the following assumptions for the modern linear regression:

1.  **Validity**

2.  Linearity

3.  Independence of errors

4.  Equal variance of errors

5.  Normality of errors

## In R

```{r, eval = TRUE}

happy = read.csv("http://www.nd.edu/~sberry5/data/happy.csv", strip.white = TRUE)

basicExample = lm(Happiness.Score ~ Economy..GDP.per.Capita., data = happy)

summary(basicExample)
```

### What Does This Mean For Me?

We can see that there is chunk of information about our residuals. This distribution of residuals is telling us how much we are off in our predictions. Our minimum is telling us the greatest over-prediction (observed - predicted) of the model and our maximum is telling us the greatest under-prediction. The median residual is offering the general model performance (are we generally under-predicting, over-predicting, or something else).

The intercept is the fitted value of Happiness.Score when everything else (i.e., our predictor coefficient) is equal to 0.

The coefficient for "Economy.GDP" is saying that for every unit increase in GDP, the average change in the mean of Happiness goes up ~2.21 units.

The standard error (**Std. Error**) is essentially providing the standard deviation of the coefficient -- it is the average distance that estimates fall from the regression line for that variable. As we see smaller standard errors, we see better predictions.

Our *t*-values, and corresponding *p*-values (**Pr(>|t|)**) provide whether or not that variable is predictive of the DV. High *t*-values and low *p*-values are going to give us significant predictors. Those *t*-values are the coefficient divided by the standard error. 

<aside>
We can determine the significance of the *t*-value by looking at its place in a distribution.
</aside>

As we go beyond the coefficients, we get into the model-based information. The *Residual standard error* gives us information about how our average amount of drift between our observed data and our predicted response. So, on average, our actual happiness scores deviate about .7174 points from our predicted scores. While this offers us a glimpse into our model's lack of fit, it is up to us to determine how bad this is as a model.

The two different $R^2$ values (**Multiple R-squared** and **Adjusted R-squared**) will tell us how much variability within our DV is accounted for by our predictor(s). As we get closer to 1, our predictors are accounting for more variability within the DV. The adjusted version is penalized for the number of predictors.

<aside>
You may encounter rules of thumb for determining how big the $R^2$ need to be -- don't listen to them. Imagine something messy, like human behavior, and imagine even explaining 10% of of the weirdness.
</aside>

Let's add another term to our model:

```{r}
twoPredictors = lm(Happiness.Score ~ Economy..GDP.per.Capita. + Generosity, data = happy)

summary(twoPredictors)
```

With another term in the model, our coefficient for GDP has changed to 2.22

- This is holding generosity constant.

Generosity has a coefficient of 1.70.

- Holding GDP constant.

What do these patterns mean?

Do they make sense?

## Let's See How This Holds Up

${happy} = \alpha + \beta_{1} gdp_{t} + \beta_{2}  generosity_{t} + \epsilon$

```{r}
twoPredictors$coefficients['(Intercept)'] + 
  twoPredictors$coefficients['Economy..GDP.per.Capita.'] * happy$Economy..GDP.per.Capita.[1] +
  twoPredictors$coefficients['Generosity'] * happy$Generosity[1]

happy$Happiness.Score[1]
```


## Visual Diagnostics

Using the plot method for our linear model will give us 4 very informative plots.

```{r}
# par(mfrow = c(2, 2))

plot(twoPredictors)
```

The "Residuals vs Fitted" plot is showing us our linear relationships within residuals -- it should appear random!

The "Normal Q-Q" plot is giving us an idea about our multivariate normality (normally-distributed errors) -- the points should hug the line.

The "Scale-Location" plot is similar to our "Residuals vs Fitted" plot, but is best for homoscedasticity detection -- again, random is great!

Finally, "Residuals vs Leverage" shows us observations exhibiting a high degree of leverage on our regression.

## A Demonstration

NOTE! This is only if you want to see the back-end of a linear model. If you do not care, just ignore this part!

```{r}
# predictors and response
N = 100 # sample size
k = 2   # number of desired predictors
X = matrix(rnorm(N*k), ncol=k)  
y = -.5 + .2*X[,1] + .1*X[,2] + rnorm(N, sd=.5)  # increasing N will get estimated values closer to these

dfXy = data.frame(X,y)

plot(dfXy$y, dfXy$X1)
```

### Long Form

```{r}
lmfuncLS = function(par, X, y){
  # arguments- par: parameters to be estimated; X: predictor matrix with intercept 
  # column, y: response
  
  # setup
  beta = par                                   # coefficients
  
  # linear predictor
  LP = X%*%beta                                # linear predictor
  mu = LP                                      # identity link
  
  # calculate least squares loss function
  L = crossprod(y-mu)
}
```

```{r}
X = cbind(1, X)

init = c(1, rep(0, ncol(X)))

names(init) = c('sigma2', 'intercept','b1', 'b2')
```


```{r}
optlmLS = optim(par = init[-1], fn = lmfuncLS, 
                X = X, y = y, control = list(reltol = 1e-8))

optlmLS$par
```


```{r}
modlm = lm(y~., dfXy)

summary(modlm)
```

### QR Decomposition

```{r}
QRX = qr(X)
Q = qr.Q(QRX) # Orthogonal matrix
R = qr.R(QRX) # Upper triangle
Bhat = solve(R) %*% crossprod(Q, y)
qr.coef(QRX, y)
```

### Pure Matrix Multiplication

```{r}
coefs = solve(t(X)%*%X) %*% t(X)%*%y
```


## Factor Variables

There is a lot of interesting data in the world. The following data comes from Chicago:

```{r}
crimeScore = read.csv("http://nd.edu/~sberry5/data/crimeScore.csv")

dplyr::glimpse(crimeScore)
```

Let's see if there might be anything interesting going on with the gender variable:

```{r}
library(dplyr)

genderScores = crimeScore %>% 
  group_by(SEX_CODE_CD) %>% 
  summarize(meanScore = mean(SSL_SCORE))

genderScores
```

Now, we can incorporate those into a regression model:

```{r}
factorTest = lm(SSL_SCORE ~ SEX_CODE_CD, data = crimeScore)

summary(factorTest)
```

These are called treatment contrasts. If you want to change the treatment, try something like the following:

```{r}
factorTest2 = lm(SSL_SCORE ~ relevel(crimeScore$SEX_CODE_CD, ref = "X"), 
                 data = crimeScore)

summary(factorTest2)
```

## Ordered Factors

```{r}
library(dplyr)

summary(as.factor(crimeScore$AGE_CURR))

crimeScore$AGE_CURR[which(crimeScore$AGE_CURR == "")] = NA

crimeScore = crimeScore %>% 
  mutate(ageRec = relevel(AGE_CURR, ref = "less than 20"), 
         ageRec = as.ordered(ageRec))
```

If we include an ordered factor in our model, we might get a surprising result:

```{r}
orderedMod = lm(SSL_SCORE ~ ageRec, data = crimeScore)

summary(orderedMod)
```

What R is returning is an *orthogonal polynomial contrast*.  We are dealing with k-1 higher-order approximations of the trends of the variable (linear, quadratic, cubic, ^4, etc.).  So in our model, we are looking at the effects of each trend level on our dependent variable.

```{r, echo = FALSE}

options(scipen = 999)

library(ggplot2)

library(dplyr)

testCoef = coefficients(lm(SSL_SCORE ~ ageRec, data = crimeScore))

C = contr.poly(7)

linearContrast = data.frame(meanScore = c(testCoef[1] + testCoef[2] * C[1, 1], 
                      testCoef[1] + testCoef[2] * C[2, 1],
                      testCoef[1] + testCoef[2] * C[3, 1], 
                     testCoef[1] + testCoef[2] * C[4, 1], 
                     testCoef[1] + testCoef[2] * C[5, 1], 
                     testCoef[1] + testCoef[2] * C[6, 1], 
                     testCoef[1] + testCoef[2] * C[7, 1]), 
                     ageRec = c("less than 20", "20-30", "30-40", 
                             "40-50", "50-60", "60-70", "70-80"))

quadraticContrast = data.frame(meanScore = c(testCoef[1] + testCoef[3] * C[1, 2], 
                      testCoef[1] + testCoef[3] * C[2, 2],
                      testCoef[1] + testCoef[3] * C[3, 2], 
                     testCoef[1] + testCoef[3] * C[4, 2], 
                     testCoef[1] + testCoef[3] * C[5, 2], 
                     testCoef[1] + testCoef[3] * C[6, 2], 
                     testCoef[1] + testCoef[3] * C[7, 2]), 
                     ageRec = c("less than 20", "20-30", "30-40", 
                             "40-50", "50-60", "60-70", "70-80"))

cubicContrast = data.frame(meanScore = c(testCoef[1] + testCoef[4] * C[1, 3], 
                      testCoef[1] + testCoef[4] * C[2, 3],
                      testCoef[1] + testCoef[4] * C[3, 3], 
                     testCoef[1] + testCoef[4] * C[4, 3], 
                     testCoef[1] + testCoef[4] * C[5, 3], 
                     testCoef[1] + testCoef[4] * C[6, 3], 
                     testCoef[1] + testCoef[4] * C[7, 3]), 
                     ageRec = c("less than 20", "20-30", "30-40", 
                             "40-50", "50-60", "60-70", "70-80"))

quarticContrast = data.frame(meanScore = c(testCoef[1] + testCoef[5] * C[1, 4], 
                      testCoef[1] + testCoef[5] * C[2, 4],
                      testCoef[1] + testCoef[5] * C[3, 4], 
                     testCoef[1] + testCoef[5] * C[4, 4], 
                     testCoef[1] + testCoef[5] * C[5, 4], 
                     testCoef[1] + testCoef[5] * C[6, 4], 
                     testCoef[1] + testCoef[5] * C[7, 4]), 
                     ageRec = c("less than 20", "20-30", "30-40", 
                             "40-50", "50-60", "60-70", "70-80"))

plotDat = crimeScore %>% 
  dplyr::select(SSL_SCORE, ageRec) %>% 
  group_by(ageRec) %>% 
  summarize(meanScore = mean(SSL_SCORE)) %>% 
  na.omit()

ggplot(plotDat, aes(ageRec, meanScore, group = 1)) +
  geom_point(size = 3, color = "#e41a1c") + # Red
  geom_point(data = linearContrast, aes(y = meanScore, group = 1, color = "#377eb8")) + # Blue
  geom_line(data = linearContrast, aes(y = meanScore, group = 1, color = "#377eb8")) +
  geom_point(data = quadraticContrast, aes(y = meanScore, group = 1, color = "#4daf4a")) + # Green
  geom_line(data = quadraticContrast, aes(y = meanScore, group = 1, color = "#4daf4a")) +
  geom_point(data = cubicContrast, aes(y = meanScore, group = 1, color = "#984ea3")) + # Purple
  geom_line(data = cubicContrast, aes(y = meanScore, group = 1, color = "#984ea3")) +
  geom_point(data = quarticContrast, aes(y = meanScore, group = 1, color = "#ff7f00")) + # Orange 
  geom_line(data = quarticContrast, aes(y = meanScore, group = 1, color = "#ff7f00")) +
  scale_color_identity("Line.Color", labels=c("Linear", "Quadratic", "Cubic", "Quartic"), guide="legend") +
  theme_minimal()
```

Here is a better example using the diamonds data from ggplot2:

```{r, echo = FALSE}
options(scipen = 999)

library(ggplot2); library(dplyr)

testCoef = coefficients(lm(price ~ cut, data = diamonds))

testCoef

C = contr.poly(5)

linearContrast = data.frame(meanPrice = c(testCoef[1] + testCoef[2] * C[1, 1], 
                      testCoef[1] + testCoef[2] * C[2, 1],
                      testCoef[1] + testCoef[2] * C[3, 1], 
                     testCoef[1] + testCoef[2] * C[4, 1], 
                     testCoef[1] + testCoef[2] * C[5, 1]), 
                     cut = c("Fair", "Good", "Very Good", 
                             "Premium", "Ideal"))

quadraticContrast = data.frame(meanPrice = c(testCoef[1] + testCoef[3] * C[1, 2], 
                      testCoef[1] + testCoef[3] * C[2, 2],
                      testCoef[1] + testCoef[3] * C[3, 2], 
                     testCoef[1] + testCoef[3] * C[4, 2], 
                     testCoef[1] + testCoef[3] * C[5, 2]), 
                     cut = c("Fair", "Good", "Very Good", 
                             "Premium", "Ideal"))

cubicContrast = data.frame(meanPrice = c(testCoef[1] + testCoef[4] * C[1, 3], 
                      testCoef[1] + testCoef[4] * C[2, 3],
                      testCoef[1] + testCoef[4] * C[3, 3], 
                     testCoef[1] + testCoef[4] * C[4, 3], 
                     testCoef[1] + testCoef[4] * C[5, 3]), 
                     cut = c("Fair", "Good", "Very Good", 
                             "Premium", "Ideal"))

quarticContrast = data.frame(meanPrice = c(testCoef[1] + testCoef[5] * C[1, 4], 
                      testCoef[1] + testCoef[5] * C[2, 4],
                      testCoef[1] + testCoef[5] * C[3, 4], 
                     testCoef[1] + testCoef[5] * C[4, 4], 
                     testCoef[1] + testCoef[5] * C[5, 4]), 
                     cut = c("Fair", "Good", "Very Good", 
                             "Premium", "Ideal"))

plotDat = diamonds %>% 
  dplyr::select(price, cut) %>% 
  group_by(cut) %>% 
  summarize(meanPrice = mean(price))

ggplot(plotDat, aes(cut, meanPrice, group = 1)) +
  geom_point(size = 3, color = "#e41a1c") + # Red
  geom_point(data = linearContrast, aes(y = meanPrice, group = 1), color = "#377eb8") + # Blue
  geom_line(data = linearContrast, aes(y = meanPrice, group = 1), color = "#377eb8") +
  geom_point(data = quadraticContrast, aes(y = meanPrice, group = 1), color = "#4daf4a") + # Green
  geom_line(data = quadraticContrast, aes(y = meanPrice, group = 1), color = "#4daf4a") +
  geom_point(data = cubicContrast, aes(y = meanPrice, group = 1), color = "#984ea3") + # Purple
  geom_line(data = cubicContrast, aes(y = meanPrice, group = 1), color = "#984ea3") +
  geom_point(data = quarticContrast, aes(y = meanPrice, group = 1), color = "#ff7f00") + # Orange 
  geom_line(data = quarticContrast, aes(y = meanPrice, group = 1), color = "#ff7f00") +
  theme_minimal()
```

From looking at the visualization, we can see how these different "approximations" can fit the data pretty well. Just as a reference: blue = linear; green = quadratic; purple = cubic; orange = quartic.

Converting them to numeric will entail a careful theoretical examination of the question at hand and the nature of the ordinal categories, but you get the nice and easier numeric interpretation that comes along with the numeric.  Converting them to factors leads us to the treatment contrasts that we used earlier.


## Practice

Let's continue using the `attrition` data from rsample. As should always be the first step, you will want to look at the structure of your data (this should be familiar from the homework):

```{r, eval = FALSE}
library(rsample)

data(attrition)

str(attrition)
```

After taking a look at your data, select a numeric variable that would be suitable for using as a dependent variable within a regression and find some variables that might predict that DV. This is your opportunity to practice the lost art of validity -- does it make sense that the predictors would have a relationship and that they would be the *cause* of the DV? 

Use the `lm()` function to test your model. How did your predictors perform? Did the coefficients point the direction you would expect? Are they significant? How did you model as a whole do?

## Interactions

We can start by looking at a multiple regression with 2 variables:

```{r}
twoVars = lm(SSL_SCORE ~ WEAPONS_ARR_CNT + NARCOTICS_ARR_CNT, data = crimeScore)

summary(twoVars)
```

Let's explore interactions (moderation to some). A key idea here is that interactions change the nature of our model. Instead of supposing that the predictor variables act in isolation on the DV, the interaction is essentially providing expanded context for our model. We are essentially saying that the values of one variable will have an effect with values of another variable on the DV. Here is what this looks like in words:

"What is the effect of X1 on Y?"

"It depends on X2's value."

```{r}
intMod = lm(SSL_SCORE ~ WEAPONS_ARR_CNT * NARCOTICS_ARR_CNT, data = crimeScore)

summary(intMod)
```

This is the model that we have estimated:

$$score = b_0 + b_1(weapons) + b_2(narcotics) + b_3(weapons*narcotics)$$

The interpretation of our main effects don't really change. 

Our interaction terms ($b_3$) is providing the amount of change in the slope of the regression of score on weapons when narcotics changes by one unit. So as narcotics arrests increase, we see an increase in the effect of weapons arrests on score (but only a tiny one).

To predict what the score value would be for certain values of weapons arrests, we could reformulate our model as:

$$score = b_0 + (b_1 + b3*narcotics)weapons + b_2(narcotics)$$



```{r}
library(effects)

modEffects = effect("WEAPONS_ARR_CNT*NARCOTICS_ARR_CNT", intMod)

plot(modEffects)
```

This is offering the relationship between weapons arrests and score at various levels of narcotics arrests.

```{r}
crimeScoreGender = crimeScore %>% 
  filter(SEX_CODE_CD != "X") %>%
  select(SSL_SCORE, SEX_CODE_CD, WEAPONS_ARR_CNT)

intMod2 = lm(SSL_SCORE ~ WEAPONS_ARR_CNT * SEX_CODE_CD, data = crimeScoreGender)

summary(intMod2)
```

Compared to women, men's score increases by 19.25 on average for each weapons arrest.

Sometimes it helps to see what is going on with a plot:

```{r}
modEffects = effect("WEAPONS_ARR_CNT*SEX_CODE_CD", intMod2)

plot(modEffects)
```

This illustrates the gender effect very nicely!

# T-tests

## What Are They Good For

You can use a *t*-test to test differences between two groups.

There are two general forms of the *t*-test:

- Independent: Test between two unrelated groups

- Paired: Used when samples are not independent of each other (e.g., this class before and after taking the course)

## Our Focus

We are going to focus mostly on comparing independent samples.

Unless you are going to be doing experimental work, you will probably not need to use paired tests.

Furthermore, you probably won't ever really need to compare a sample to the population (requires you to know $\mu$)

## Tails

Like many other tests, the *t*-test can be tested with either one tail or two tails.

Alternative hypotheses can be any one of the following:

- $\neq$

- $>$

- $<$

What is the difference?

-  Are you specifying the direction of your hypothesis or not?

### One Or Two

In all seriousness, let's consider the following plot:

```{r, eval = TRUE}
hist(rnorm(100000))
```

In a two-tailed test, I am going to split the probability of finding the effect across both extreme ends of the distribution. In a one-tailed test, I will put all of my probability of finding an effect in the specified tail.

## Let's Give It A Try

```{r}
t.test(crimeScoreGender$SSL_SCORE ~ crimeScoreGender$SEX_CODE_CD,
       alternative = "two.sided")
```

Try it with different values for alternative and with var.equal = TRUE

# Analysis Of Variance

## ANOVA

ANOVA is a lot like a *t*-test, but you can have more than two groups.

## Trying It Out

```{r}
anovaTest = aov(SSL_SCORE ~ as.factor(ageRec), data = crimeScore, projections = TRUE)

summary(anovaTest)
```

We now know that differences exist, but which groups are different?

We use Tukey's Honestly Significant Difference test:

```{r}
TukeyHSD(anovaTest)
```

# What To Do?

## Which Is The Appropriate Method?

Hopefully, we can see that these are all *essentially* identical.

We need to think about what exactly we are doing:

- Are we predicting something?

- Are we concerned about group differences?

- Do we want to be limited?

- Are we doing experimental work?

# Effect Sizes

Effect sizes, in conjunction with our *p*-values, will provide a really good idea about the strength of the difference.

With regard to effect sizes, you will most commonly come across Cohen's *d* -- it is generally used for *t*-tests.

Computationally, it is pretty simple:

$$ \frac{\mu_1 - \mu_2}{\sigma}$$

We are subtracting the mean of one group from another and dividing by the standard deviation.

```{r}
library(dplyr)

crimeScoreGender %>%
  group_by(SEX_CODE_CD) %>%
  summarize(mean = mean(SSL_SCORE),
            sd = sd(SSL_SCORE),
            n = n())

sd(crimeScoreGender$SSL_SCORE)
```


We can do it by hand:

```{r}
(283.46-278.689) / 57.99564
```


Or use things already built:

```{r}
library(compute.es)

tes(t = 23.674, n.1 = 96307, n.2 = 302320)

mes(m.1 = 283.46, m.2 = 278.689,
    sd.1 = 52.74889, sd.2 = 59.52397,
    n.1 = 96307, n.2 = 302320)
```

# Power Analysis

Rules of thumb have been around for a long time and have changed over the years -- maybe you learned that you needed 20 rows per predictor, or maybe even 50 rows per predictor. Instead of trusting outdated advice, use actual science to determine how many people you need to find if a difference exists.

We need three of the following parameters:

-  Effect size

-  Sample size

-  Significance level

-  Power

We **should** always be doing this *a priori*

-  Sometimes, it is fun to be a "statistical coroner"

## Power

Power is ability to detect an effect. In NHST words, we are trying to determine if we correctly reject the null hypothesis.

- Type I errors: Reject a true $H_{o}$ (false positive -- saying something is there when it is not)

- Type II errors: Reject a false $H_{o}$ (false negative -- saying something is not there when it is)


## Putting It All Together

Let's use the <span class="func">pwr</span> package.

```{r}
library(pwr)

pwr.f2.test(u = 1, v = NULL, f2 = .05, power = .8)
```

In the function:

- u is the numerator df (*k* - 1)

- v is the denominator df (*n* - *k*)

- f2 is signficance level

- \(\Pi = 1 -\beta\)

- \(\beta = Type\,II_{prob}\)

Power is typically set at .8, because it represents a 4 to 1 trade between Type II and Type I errors.


## Different Test, Different Power Tests

We just did a test for a linear regression model.

Here is one for a *t*-test:

```{r}
tPower = pwr.t.test(n = NULL, d = 0.1, power = 0.8,
                    type = "two.sample", alternative = "greater")

plot(tPower)
```


# Main Messages

1.  If your outcome variable is a continuous numeric variable, then regression may very well be a great tool.

2.  Any type of variable can serve as a predictor variable

3.  While the linear regression, *t*-tests, and ANOVAs are all related, the linear regression is a far more flexible tool.

4.  Always interpret your coefficients in your regression models before looking at the significance.

5.  Don't trust rules of thumb for sample size -- use a power analysis instead!