---
title: "Resampling"
description: |
  Various Methods for Inference
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# A Standard Model

At this point, we have been using single models pretty regularly. Let's fit a simple model:

```{r}
library(data.table)
library(ggplot2)

turnoverData <- fread("http://www.nd.edu/~sberry5/data/turnoverData2.csv")

slim <- lm(startingSalary ~ age + numberPriorJobs, data = turnoverData)

summary(slim)

turnoverData$fittedValue <- slim$fitted.values

turnoverData$residValue <- slim$residuals
```

We will just tuck that values away in our data for now, but we will return to them.

# Resampled Models

We have spoken a few times about what all of our test statistics are based upon: repeated samples from the population. Unfortunately, we rarely have the time or resources to actually do this. As luck would have it, though, we can treat our sample as a population (in theory, the sample should match the shape of our population if drawn at random). If it is a population, then we can repeatedly sample from that population. When we resample from our "population", we can start to get a general sense of how accurate our estimates might be (we are not improving the estimate).

This resampling can take on a few different forms and those different forms can answer slightly different questions. We are going to discuss three specific techniques: *bootstrapping*, *subsampling*, and *permutation*. Do keep in mind, though, that there are many different forms of resampling.

This is also highly-related to something you are doing in machine learning, so try to think about what that might be.

<aside>
You might also encounter the jack-knife method, which has analogs to machine learning techniques.
</aside>

## A Brief Demonstration of infer

Before we get too far, let's use the `infer` package to get a sense of the possible range of values the mean of *startingSalary* could take under resampled conditions.

```{r}
library(infer)

observedMean <- mean(turnoverData$startingSalary)

bootDist <- turnoverData %>% 
  specify(response = startingSalary) %>% 
  hypothesize(null = "point", mu = 90000) %>% 
  generate(1000, type = "bootstrap") %>% 
  calculate(stat = "mean")

visualise(bootDist) +
  shade_p_value(obs_stat = observedMean, direction = NULL) +
  shade_confidence_interval(endpoints = get_ci(bootDist)) +
  theme_minimal()
```

While this serves as a very nice little demonstration, we need to utilize something a bit more substantial than this package.

<aside>
There are several boostrapping packages in R. You will probably have the best luck with just writing your own functions.
</aside>

## Bootstraps

Bootstraps are not a new technique, but they are easier to use now compared to when they were first introduced (computers have gotten faster). Bootstrapping involves taking full samples out of the population. The trick here, though, is that the sampling happens with replacement. This means that the same observation can appear in the bootstrapped data more than once and that is by design. 

<aside>
While not the original, this <a href="https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815">article</a> by Efron and Tibshirani is excellent.
</aside>

Bootstrap samples provide some pretty cool advantages. Perhaps the biggest is that the underlying distribution of the variables does not need to be normal. As we generate a statistic for each of our samples, we can almost guarantee that our resulting distribution will be normal. We can also easily get the 95% confidence interval of the distribution of test statistics.

In the regression setting, we might be interested in the *t*-values, the coefficients, or even the standard errors. We can compare group means with *t*-tests or even look at the differences. The moral of the story is that we can bootstrap any test statistic that we would like.

```{r}
modelVars <- turnoverData[, 
                          .(startingSalary, age, numberPriorJobs)]

bootstrapping <- function(df) {
  df <- df
  
  sampledRows <- sample(1:nrow(df), nrow(df), replace = TRUE)
  
  df <- df[sampledRows, ]
  
  bsMod <- lm(startingSalary ~ age + numberPriorJobs, data = df)
  
  results <- broom::tidy(bsMod)
  
  return(results)
}

bootstrapping(modelVars)

bsRep <- replicate(1000, bootstrapping(modelVars), 
                   simplify = FALSE)

bsCombined <- do.call("rbind", bsRep)

hist(bsCombined$statistic[bsCombined$term == "age"], 
     col = "black")

abline(v = summary(slim)$coefficients["age","t value"], 
       col = "cornflowerBlue", lwd = 2)

hist(bsCombined$statistic[bsCombined$term == "numberPriorJobs"], 
     col = "black")

abline(v = summary(slim)$coefficients["numberPriorJobs","t value"], 
       col = "cornflowerBlue", lwd = 2)
```

Let's also find the 95% confidence interval and plot them for age. 

```{r}
meanEffect <- mean(bsCombined$statistic[bsCombined$term == "age"])

ciUpper <- quantile(bsCombined$statistic[bsCombined$term == "age"], .975)

ciLower <- quantile(bsCombined$statistic[bsCombined$term == "age"], .025)

hist(bsCombined$statistic[bsCombined$term == "age"], 
     col = "slategray1")

abline(v = summary(slim)$coefficients["age","t value"], 
       col = "goldenrod4", lwd = 2)

abline(v = ciUpper, col = "sienna3", lwd = 2)

abline(v = ciLower, col = "sienna3", lwd = 2)

abline(v = meanEffect, col = "sienna3", lwd = 2)
```

So what does this mean in practice? First, we are getting a sense of our estimate's precision. Across all of our resampled values, 95% of them will contain the true population value between `r ciLower` and `r ciUpper`.

<aside>
Here is a really nice <a href="http://users.stat.umn.edu/~helwig/notes/bootci-Notes.pdf">presentation</a> of bootstrap confidence intervals.
</aside>

Let's check out the effect for `numberPriorJobs`:

```{r}
meanEffect <- mean(bsCombined$statistic[bsCombined$term == "numberPriorJobs"])

ciUpper <- quantile(bsCombined$statistic[bsCombined$term == "numberPriorJobs"], 
                    .975)

ciLower <- quantile(bsCombined$statistic[bsCombined$term == "numberPriorJobs"], 
                    .025)

hist(bsCombined$statistic[bsCombined$term == "numberPriorJobs"], 
     col = "slategray1")

abline(v = summary(slim)$coefficients["numberPriorJobs","t value"], 
       col = "goldenrod4", lwd = 2)

abline(v = ciUpper, col = "sienna3", lwd = 2)

abline(v = ciLower, col = "sienna3", lwd = 2)

abline(v = meanEffect, col = "sienna3", lwd = 2)
```

For our `numberPriorJobs` variable, 95% of the intervals contain the true *t*-value that is somewhere between 0 and 4. For extra information and added clarity, let's look at the coefficient:

```{r}
meanEffect <- mean(bsCombined$estimate[bsCombined$term == "numberPriorJobs"])

ciUpper <- quantile(bsCombined$estimate[bsCombined$term == "numberPriorJobs"], 
                    .975)

ciLower <- quantile(bsCombined$estimate[bsCombined$term == "numberPriorJobs"], 
                    .025)

hist(bsCombined$estimate[bsCombined$term == "numberPriorJobs"], 
     col = "slategray1")

abline(v = summary(slim)$coefficients["numberPriorJobs","Estimate"], 
       col = "goldenrod4", lwd = 2)

abline(v = ciUpper, col = "sienna3", lwd = 2)

abline(v = ciLower, col = "sienna3", lwd = 2)

abline(v = meanEffect, col = "sienna3", lwd = 2)
```

We have a 95% confidence interval that ranges from near 0 to close to 30. With my *t*-value distribution and the distribution of the coefficients, I might not be super confident about the number of prior jobs being a significant predictor for starting salary; in other words, we are seeing a fair amount of variance.

Just for fun, let's do age's coefficient with ggplot2:

```{r}
library(ggplot2)

meanEffect <- mean(bsCombined$estimate[bsCombined$term == "age"])

ciUpper <- quantile(bsCombined$estimate[bsCombined$term == "age"], .975)

ciLower <- quantile(bsCombined$estimate[bsCombined$term == "age"], .025)

ggplot(bsCombined[bsCombined$term == "age", ], aes(estimate)) +
  geom_histogram(fill = "slategray1") +
  geom_vline(xintercept = ciLower, color = "sienna2") +
  geom_vline(xintercept = ciUpper, color = "sienna2") +
  geom_vline(xintercept = summary(slim)$coefficients["age","Estimate"], 
             color = "goldenrod4") + 
  theme_minimal()
```

### Residual Bootstrapping

Shuffling data around is pretty easy. After running our first model, we stuck our predicted values and our residuals back into the original data. We did this because we are going to use them for a specific type of bootstrap: residual bootstrapping. 

Clearly the first step was to create the model. Next, we are going to add a random residual to a specific predicted value -- this becomes our new outcome variable.

```{r}
randomResid <- function(df) {
  df <- df
  
  sampledRows <- sample(1:nrow(df), nrow(df), replace = FALSE)
  
  df$randomResid <- df[sampledRows, "residValue"]
  
  df$yStar <- df$fittedValue + df$randomResid
  
  residMod <- lm(yStar ~ age + numberPriorJobs, data = df)
  
  results <- broom::tidy(residMod)
  
  return(results)
}

randomResid(turnoverData)

residRep <- replicate(1000, randomResid(turnoverData), 
                      simplify = FALSE)

residCombined <- do.call("rbind", residRep)

hist(residCombined$statistic[residCombined$term == "age"], 
     col = "black")

abline(v = summary(slim)$coefficients["age","t value"], 
       col = "cornflowerBlue", lwd = 2)

hist(residCombined$statistic[residCombined$term == "numberPriorJobs"], 
     col = "black")

abline(v = summary(slim)$coefficients["numberPriorJobs","t value"], 
       col = "cornflowerBlue", lwd = 2)
```

We largely get the same results as we did with our boostrapping. Why might we want to perform this type of bootstrap? It put less trust in the model. This type of resampling approach would guess that the original regression model has the proper form (nothing really changes on the predictor variable side), but has no assumptions about how the residuals are distributed. It also removes the influence of any extreme scores within the model.

## Subsample

Using subsampling is very similar to bootstraps, but with some key exceptions. The first is right in the name; we are going to draw a smaller sample out of our data. We are also not going to use replacement. 

How much of a sample you take is up to you, but don't go too small or large.

```{r}
bootstrapping <- function(df, type = c("bs", "subsample")) {
  df <- df
  
  sampledRows <- switch(type, 
                        bs = sample(1:nrow(df), nrow(df), 
                                    replace = TRUE), 
                        subsample = sample(1:nrow(df), round(nrow(df) * .75), 
                                           replace = FALSE))
  
  df <- df[sampledRows, ]
  
  bsMod <- lm(startingSalary ~ age + numberPriorJobs, data = df)
  
  results <- broom::tidy(bsMod)
  
  return(results)
}

ssRep <- replicate(1000, bootstrapping(modelVars, type = "subsample"), 
                   simplify = FALSE)

ssCombined <- do.call("rbind", ssRep)

hist(ssCombined$statistic[ssCombined$term == "age"], 
     col = "black")

abline(v = summary(slim)$coefficients["age","t value"], 
       col = "cornflowerBlue")

hist(ssCombined$statistic[ssCombined$term == "numberPriorJobs"], 
     col = "black")

abline(v = summary(slim)$coefficients["numberPriorJobs","t value"], 
       col = "cornflowerBlue", lwd = 2)
```

Just like we did before, let's see the CIs.

```{r}
mean(ssCombined$statistic[ssCombined$term == "age"])

quantile(ssCombined$statistic[ssCombined$term == "age"], 
         .975)

quantile(ssCombined$statistic[ssCombined$term == "age"], 
         .025)
```


## Permutation

The purpose is the same, but the mechanism is different. Instead of sampling our data, we are going to create random shuffles of our data (specifically, our outcome variable). We don't have to have every single permutation present for our circumstances, but it is possible for small experimental design settings. 

The primary goal here is to create a distribution (just like we did before), but we are creating a *null distribution*. A reasonable question is why we would ever want to do this type of thing. The best reason is the start of proof. If the null distribution is real, you should never find the hypothesized effect (or almost never). If you shuffle your dependent variable around, you should observe very few effects.

<aside>
Maybe you have seen something similar with visualizations?
</aside>

```{r}
permuteBS <- function(df) {
  df <- df
  
  sampleValues <- sample(1:nrow(df), nrow(df), replace = TRUE)
  
  df$startingSalary <- df[sampleValues, "startingSalary"]
  
  bsMod <- lm(startingSalary ~ age + numberPriorJobs, data = df)
  
  results <- broom::tidy(bsMod)
  
  return(results)
}
```

```{r}
bsRep <- replicate(1000, permuteBS(modelVars), 
                   simplify = FALSE)

bsCombined <- do.call("rbind", bsRep)

hist(bsCombined$statistic[bsCombined$term == "age"])

hist(bsCombined$statistic[bsCombined$term == "numberPriorJobs"], 
     col = "black")

abline(v = summary(slim)$coefficients["numberPriorJobs","t value"], 
       col = "cornflowerBlue", lwd = 2)
```

Our observed effect is way out in the tail of our null distribution: this gives a pretty clear idea that we can probably reject the null for this model.

We can also derive a one-tailed *p*-value for our null distribution:

```{r}
(sum(bsCombined$statistic[bsCombined$term == "numberPriorJobs"] > summary(slim)$coefficients["numberPriorJobs","t value"])) / nrow(bsCombined)

(sum(bsCombined$statistic[bsCombined$term == "age"] > 
       summary(slim)$coefficients["age","t value"])) / nrow(bsCombined)
```

Or a two-tailed *p*-value:

```{r}
(sum(abs(bsCombined$statistic[bsCombined$term == "numberPriorJobs"]) > summary(slim)$coefficients["numberPriorJobs","t value"])) / nrow(bsCombined)

(sum(abs(bsCombined$statistic[bsCombined$term == "age"]) > summary(slim)$coefficients["age","t value"])) / nrow(bsCombined)
```

From our null distributions, we can safely reject the null for age's effect on starting salary. We also get very low *p*-values for the number of jobs.

## Computational Power

When running our bootstrapping functions, things were generally going pretty fast; running 1000 replications is no problem. However, we probably won't be doing this just 1000 times (or maybe even with a single model). We will probably be doing it many thousands of times. If we are going to be running that many simulations, we probably want to get full use of our machines. 

We can do that by running our models in parallel.

```{r, eval = FALSE}
timeRunner <- function(repNumber) {
  replicate(repNumber, randomResid(turnoverData), simplify = FALSE)
}

library(parallel)

cl <- parallel::makeCluster(detectCores() - 1)

clusterExport(cl, c("turnoverData", "timeRunner", "randomResid"))

clusterEvalQ(cl, library(dplyr))

t1 <- proc.time()
residOut <- clusterEvalQ(cl, timeRunner(500))
proc.time() - t1

stopCluster(cl)

residOut <- purrr::flatten(residOut) %>% 
  bind_rows(.)

t1 <- proc.time()
test4 <- timeRunner(3500) # Will produce the same number of bootstraps as above.
proc.time() - t1

test4 <- do.call("rbind", test4)
```

An alternative way is as follows:

```{r, eval = FALSE}
timeRunner <- function(repNumber) {
  replicate(repNumber, expr = {
    out <- randomResid(turnoverData)
    out$number <- repNumber
    return(out)
  }, simplify = FALSE)
}

cl <- parallel::makeCluster(detectCores() - 1)

clusterExport(cl, c("turnoverData", "timeRunner", "randomResid"))

clusterEvalQ(cl, library(dplyr))

t1 <- proc.time()
residOut <- parLapply(cl, 1:141, function(x) timeRunner(x))
proc.time() - t1

residOut <- purrr::flatten(residOut) %>% 
  bind_rows(.)
```

Or...

```{r, eval = FALSE}
t1 <- proc.time()
residOut <- parSapply(cl, 1:141, function(x,...) {
  timeRunner(x)
}, simplify = FALSE)
proc.time() - t1

residOut <- purrr::flatten(residOut) %>% 
  bind_rows(.)

stopCluster(cl)
```

The parallel apply statement are a bit slower than what we previously used when we just evaluated the statement, but you should keep these in mind because they are very helpful if you are doing anything that a `for` loop might normally do.

Using the parallel package is great, but the future is with `future`: the goal is to provide a streamlined approach to parallelization, without any OS-related problems.

```{r}
library(future)

plan(multiprocess, workers = availableCores() - 1)

futureReps <- future(replicate(1000, randomResid(turnoverData), 
                               simplify = FALSE))

t1 <- proc.time()
testOut <- do.call("rbind", value(futureReps))
proc.time() - t1

t1 <- proc.time()
oldReps <- replicate(1000, randomResid(turnoverData), 
                     simplify = FALSE)
proc.time() - t1
```

As you would expect, we are hitting a speedup of around 8X, with very little in the way of code!
