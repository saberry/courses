---
title: |  
      | Unstructured Data Analytics
      | ITAO70250
header-includes:
   - \usepackage{bbm}
   -  \hypersetup{
    colorlinks=true,
    linkcolor=magenta,
    filecolor=magenta,      
    urlcolor=magenta}
  
output:
    pdf_document
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Office: 337 Mendoza

# Email: seth.berry@nd.edu

# Office Hours

M -- 12:30 to 1:30

W -- 10:00 to 12:00

F -- 10:00 to 1:00

We can also always find mutual times that will work.

# Class Days and Time

Section 1: TR, 8:00 to 9:50

Section 2: TR, 10:00 to 11:50

Location -- Stayer B003

# Course Description

Huge amounts of the world’s data is unstructured. Developing competency in how to harness this type of data in order to develop critical insights has significant value for today’s business. This course introduces the fundamental concepts of unstructured data analytics, from data acquisition and preparation to applying supervised and unsupervised machine learning approaches such as text analysis and summarization, text recognition and classification, sentiment analysis, topic modeling, and image classification. In the context of unstructured data analytics, students will also be introduced to the principles behind such classic machine learning algorithms such as naive bayes, support vector machines, and neural networks.

# Learning Goals

By successfully completing this course, you will fulfill the following objectives:

- Gain a foundational understanding of both supervised and unsupervised machine learning approaches to unstructured data.

- Develop an applied knowledge of some of the common unstructured data acquisition, exploration, and preparation approaches using R and Python.

- Understand the theoretical concepts behind text summarization, sentiment analysis, topic modeling, naive bayes, neural networks, and support vector machines.

- Develop an applied knowledge of how to implement the approaches discussed in the course using R and Python.
  

# Readings

There is no official textbook for this course, but here are some good resources:

\href{https://www.tidytextmining.com/}{Text Mining with R}

\href{https://r4ds.had.co.nz/}{R for Data Science}

\href{https://swcarpentry.github.io/r-novice-inflammation/02-func-R/}{Creating Functions}

Additional resources will be linked within course notes and on Sakai.

# Homework

During the course of the mod, we will have 3 homework assignments (worth 60, 60, and 80 points). All homework assignments must be submitted in a compiled file (knitted from R Markdown or a Python-flavored notebook of your preference) -- no other file types will be accepted and reminders won't be given. 

Homework will be composed of three distinct parts -- initial submission, individual feedback session, second submission -- and each will count towards 1/3 of the points. The initial submission must be completed before any feedback can be offered. While you do have flexibility with regard to completing the homework assignments, pushing these until the end of the mod is not advisable.

# Presentations

As opposed to a final exam, we will be having presentations on our last day of class. You can work individually or as duos. These presentations are not to exceed 4 minutes. The goal is to answer a question or solve a problem. Presentation guidelines will follow, but general creativity and appropriate technique use will figure heavily into your grade. This is a chance for you to find interesting data, not just go with what might be easy on Kaggle. 

# Engagement

Engagement is not just coming to class, but being an active participant. Throughout class, you will be given the opportunity to practice content. At the end of each class, you need to turn in your code (it does not need to be pretty and can just be any text-based file). Each submission is worth 10 points for up to a maximum of 100 points.

# Grade Breakdown

Engagement -- 100 points (25%)

Homework -- 200 points (50%)

Presentation -- 100 points (25%)

Total -- 400 points

# Schedule

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(dplyr)

data.frame(Week = c(1, "", 2, "", 3, "",
                    4, "", 5, "", 6, "", 
                    7, ""),
           Date = c("02/02 (T)", 
                    "02/04 (R)", 
                    "02/09 (T)", 
                    "02/11 (R)", 
                    "02/16 (T)", 
                    "02/18 (R)", 
                    "02/23 (T)", 
                    "02/25 (R)", 
                    "03/02 (T)", 
                    "03/04 (R)", 
                    "03/09 (T)",
                    "03/11 (R)",
                    "03/16 (T)", 
                    "03/18 (R)"), 
           Topic = c("Introduction & Programming",
                     "Regular Expressions",
                     "Data Collection (1a)", 
                     "Data Collection (1b)",
                     "Text Analysis (2)", 
                     "Sentiment Analysis",
                     "Lab 1 (3)",
                     "Topic Modeling (4)",
                     "Text Classification (5)",
                     "Lab 2 (6)",
                     "Optical Character Recognition (7)", 
                     "Image Classification (8)",
                     "Presentation Prep",
                     "Presentations"), 
           Homework = c("", 
                        "", 
                        "", 
                        "Homework #1",
                        "",
                        "",
                        "Homework #2",
                        "",
                        "",
                        "Homework #3",
                        "",
                        "", 
                        "", "")) %>% 
  knitr::kable(., format = "latex")
```

 
1a.   Using APIs

1b.   Web scraping 

2.    Term frequency, inverse document frequency, part of speech tagging, and relationships

3.    Practicum on text collection, exploration, and preparation

4.    Latent Semantic Analysis, Latent Dirichlet Allocation, and NNMF

5.    Naive Bayes for document classification

6.    Practicum on text analysis

7.    Support Vector Machines and their application to identifying text (OCR)

8.    Artificial Neural Networks and image classification.