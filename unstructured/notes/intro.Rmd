---
title: "Unstructured Data Analytics"
description: |
  Introduction and Data
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# What Is Unstructured Data?

The term *unstructured data* is rife with wackiness and will certainly shift with time. We can, however, work from a place where we just consider unstructured data to be data that does not nicely fit into a pre-defined tabular manner. If we take this consideration to a logical end, then we can see that many forms of data might come to us as unstructured. However, we need to impose structure on our data so that we can achieve our goals (whatever they might be).

<aside>
Once we get rolling, it will become clear that most data is unstructured to various degrees.
</aside>

And this is where the challenge of unstructured data rests -- getting that data into a usable form. Some data that we consider to be unstructured is structured (JSON, XML, etc.), but not in an immediately useful manner. 

When thinking about unstructured data, it also helps to put ourselves into a creative frame of mind. In no other part of the analytics world will you have to think through puzzle-like solutions like you will with unstructured data. The reward, however, is that you walk away with a skill set that will allow you to create insights from situations/organizations that are otherwise "data rich, but information poor". 

# Data Types

Throughout our time together, we will be messing around with a few different types of data. Often, that data will be converted into some numeric format for actual computation to occur. In other instances, we will be able to use the data directly. 

## Text

Text is likely to be what you will find the most (and will be our biggest focus). The reason for the focus is simply what can be done with it and the vastness of what is available. Even in the day where we are thinking towards massive amounts of audio being recorded, we are still converting that to text most of the time. 

A great deal of insight can be gathered from text: what are people talking about, how do those people feel about the stuff they are talking about, are multiple texts generated by different people or the same people. Natural Language Processing (NLP) is all the rage, but even more simple approaches can be immensely helpful for organizations.

The great thing about text is that we can treat it like we would most any other variable in R; aside from any cleaning, we can just treat it like a string.

## Images

You may not believe this, but we are at the point now where there are tons of images floating around online. Not only do people put an obscene number of pictures of themselves online, but we also have an incredible number of images from computers (drones, satelittes etc.). 

Images, however, will prove to be a little bit more difficult to work on. This difficulty rests mostly with what an image actually is -- a collection of pixels in various combinations of RGB space. Before we do anything with actual analyses, we will need to do a fair amount of preprocessing for any image that we will want to use.

# Acquisition & Types

Where exactly do we find these data types? All over the place. The location of the data does not (generally) pose an issue for us; instead, we primarily need to be concerned about what form the data takes.

## Hypertext Markup Language

Where would the web be without HTML? Probably a much different place. For us, though, html is a great place to find data. Life is easy when we are pulling data out of html tables:

```{r}
library(rvest)

songs = read_html("https://en.wikipedia.org/wiki/List_of_Hot_Country_Songs_number_ones_of_2005") %>% 
  html_table() %>% 
  `[[`(1) %>% 
  rmarkdown::paged_table()

songs
```


What about this <a href="https://www.billboard.com/charts/country-songs">page</a>? How would we go about getting that data? We are going to need to do some digging to find what we need here:


```{r}
test = read_html("https://www.billboard.com/charts/country-songs") %>% 
  html_nodes(".chart-list-item__text-wrapper") %>% 
  html_text() %>% 
  gsub("\n*Song Lyrics\n*|^\n+|\n+\\s*$", "", .) %>% 
  data.frame(title = regmatches(., regexpr(".*[?=\n]", .)), 
             artists = regmatches(., regexpr("", .)))
```


### Cascading Style Sheets

Before CSS, the web was styled purely with HTML -- and what a style it was! While we might not really care about style for our purpose, CSS can help find stuff within a webpage. CSS has a wide variety of selectors, combinators, pseudo-classes, and pseudo-elements that we can use to pull data off of webpages.

Whenever you are trying to pull data off of a page, it is always a good idea to inspect the source for any hints offered. Let's say that I want to do some headhunting for an analytics department and we want to find some <a href="https://psychology.nd.edu/graduate-programs/areas-of-study/quantitative/area-faculty/">Quant Psych faculty</a> who might be worth a visit. We also want to know where those people went to school. We might also want to grab everyone else, but we don't need to know their educational background.

```{r}
facultyData = read_html("https://psychology.nd.edu/faculty/faculty-by-alpha/") %>% 
  html_nodes(".columns.medium-9.large-10")

faculty = lapply(1:length(facultyData), function(x) {
  
  out = tryCatch({
    name = facultyData[x] %>% 
      html_nodes("h3") %>% 
      html_text()
    
    area = facultyData[x] %>% 
      html_nodes(".faculty-areas ul li") %>% 
      html_text()
    
    email = facultyData[x] %>% 
      html_nodes(".faculty-email") %>% 
      html_text()
    
    webpage = facultyData[x] %>% 
      html_nodes("h3 a") %>% 
      html_attr("href") %>% 
      paste("https://psychology.nd.edu/", ., sep = "")
    
    if(area == "Quantitative") {
      
      education = read_html(webpage) %>% 
        html_nodes(".faculty-education p") %>% 
        html_text()
      
      allFaculty = data.frame(name = name, 
                              area = area, 
                              education = education,
                              email = email,
                              webpage = webpage,
                              stringsAsFactors = FALSE)
    } else {
      allFaculty = data.frame(name = name, 
                              area = area, 
                              education = NA,
                              email = email,
                              webpage = webpage,
                              stringsAsFactors = FALSE)
    }
  }, error = function(e) {
    return(NA)
  })
  
  return(out)
})


```


## JavaScript Object Notation

More and more, we are seeing JavaScript Object Notation (JSON) creep up everywhere on the web, because it is designed to facilitate smooth information transfer between a server and an individual's browser. When nice data gets served up to a fancy table or visualization on the web, it is probably coming through in JSON. An added benefit (not for us), but this data cannot really be scraped anymore; instead, we often need to look through the network structure find where it is located and then read it in directly (note -- it cannot always be found).

```
'[
  "firstName": "Seth",
  "lastName": "Berry",
  "isAlive": true,
  "age": 33,
  "address": {
    "streetAddress": "337 Mendoza",
    "city": "Notre Dame",
    "state": "IN",
    "postalCode": "46566"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "217 884-2303"
    },
    {
      "type": "office",
      "number": "574 631-0018"
    },
    {
      "type": "mobile",
      "number": "217 822-0018"
    }
  ],
  "children": [],
  "spouse": null
]'
```

Here is how we can read json right into R:

```{r, eval = FALSE}
jsonlite::fromJSON("https://api.dibitnow.com/api/v1/eventPerformers/getEventPerformers?eventId=1")
```


Simple json is not really very difficult to parse; however, json can be incredibly nested, so you need to do a lot of flattening and joining.


## Extensible Markup Language

Extensible markup language (XML) was created with the intention of being both human and machine readable -- and it generally fits that bill. Querying XML, however, tends to require the use of XPaths. XPaths are situated somewhere along the lesser circles of Hell, but can be used to great effectiveness when trying to pull stuff out of XML.

```
<nutrition>
  <daily-values>
    <total-fat units="g">65</total-fat>
    <saturated-fat units="g">20</saturated-fat>
    <cholesterol units="mg">300</cholesterol>
    <sodium units="mg">2400</sodium>
    <carb units="g">300</carb>
    <fiber units="g">25</fiber>
    <protein units="g">50</protein>
  </daily-values>
  <food>
    <name>Avocado Dip</name>
    <mfr>Sunnydale</mfr>
    <serving units="g">29</serving>
    <calories total="110" fat="100"/>
    <total-fat>11</total-fat>
    <saturated-fat>3</saturated-fat>
    <cholesterol>5</cholesterol>
    <sodium>210</sodium>
    <carb>2</carb>
    <fiber>0</fiber>
    <protein>1</protein>
    <vitamins>
      <a>0</a>
      <c>0</c>
    </vitamins>
    <minerals>
      <ca>0</ca>
      <fe>0</fe>
    </minerals>
  </food>
</nutrition>
```



```{r}
library(xml2)

mainPage = xml2::read_html("https://psychology.nd.edu/faculty/faculty-by-alpha/")

facultyNodes = xml_find_all(mainPage, '//*[@class="faculty-item"]/*[@class="columns medium-9 large-10"]')

nameNode = xml_find_all(facultyNodes, '//h3/a/text()')

xml_find_all(facultyNodes, '//*[@class="faculty-areas"]/ul')
```

While they still have a place in the world, that is all the further we will go on the xpath front. Most of what you will need to do can be handled by the CSS selectors.

# Methods

We are going to see several different methods during the next several weeks. We will go from dimension reduction (LSA, LDA, SVD) to Naive Bayes to Neural Networks, with stops in between. A great many of these techniques will focus on classification (e.g., what topic does a text belong to, is that an image of a person or a monster), but many have extensions to regression-focused problems. 

With regard to depth, it will vary for each topic. We will have a pretty good idea about how we would do something like a Naive Bayes, PCA, or Neural Network by hand; for other topics, though, we won't get too crazy. The biggest reason for this is due in large part to how you might actually engage with such models. If you are going to be working on an image classification problem, the routines/architectures are already pretty well established for reasonable image classification. Largely, the focus will be on application and interpretation.

Let's work through an example. Let's define two objects:

```{r}
N <- 1000 # sample size
k <- 1  # variables
X <- matrix(rnorm(N * k), ncol = k)  
Y <- -.5 + .2 * X[, 1] + rnorm(N, sd = .5)  

head(X)

head(Y)
```

Now we can do some matrix multiplication. Let's take the transpose of X, multiply it by the original X, and solve the resultant matrix:

```{r}
transposeX <- t(X)

multiplyX <- transposeX %*% X

solveX <- solve(multiplyX) # Functionally equivalent to 1 / multiplyX

solveX
```

Next, let's take that transposed X and multiply it by Y:

```{r}
multiplyY <- transposeX %*% Y

multiplyY
```

Finally, we can take our solveX and multiply it by the multiplyY matrix:

```{r}
solveX %*% multiplyY
```

Let's do something else with our X and Y objects: do some linear algebra and perform a Gram–Schmidt orthogonalization of the X matrix (this is also called QR decomposition).

Every single X matrix can be broken down to the QR matrices

Q is going to be $N * p$ - the same dimension as the original. 

R is going to be $p * p$ - the number of columns by the number of columns with 0's below the diagonal.

The Q matrix will be an orthogonal matrix (remember that orthogonal means that the components will not be correlated -- $Q^T * Q = I$) and the R matrix is the upper triangle.

```{r}
QRX <- qr(X)
Q <- qr.Q(QRX) # Orthogonal matrix
R <- qr.R(QRX) # Upper triangle
Bhat <- solve(R) %*% crossprod(Q, Y)
Bhat
```

<a href="https://rpubs.com/aaronsc32/qr-decomposition-gram-schmidt">Here</a> is a nice demo of the process.

Finally, let's try this:

```{r}
meanX <- mean(X)

meanY <- mean(Y)

sdX <- sd(X)

sdY <- sd(Y)

corXY <- cor(X, Y)

corXY * (sdY / sdX)

meanY - ((corXY * (sdY / sdX)) * meanX)
```

How comfortable do you feel using and interpreting a linear regression? At this point, you likely feel reasonably comfortable with it; you can use it and feel comfortable using even without knowing all of that stuff that is going on in the background. The background can come with time. 

```{r}
lm(Y ~ X)
```

# Software

What is best for the task?

One issue with deep networks is the computational demand -- they are not for the weaker machines. If you have an Nvidia GPU, you can make good use of it; otherwise, you will be running these things on your CPU and waiting.